{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse import coo_matrix, csr_matrix, vstack\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "assert(torch.cuda.is_available())\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = torch.load('build/extracted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Str2idx():\n",
    "    def __init__(self, myset) -> None:\n",
    "        self.idxDict = {}\n",
    "        idx = 0\n",
    "        for k in myset:\n",
    "            self.idxDict[k] = idx\n",
    "            idx += 1\n",
    "\n",
    "    def __call__(self, query):\n",
    "        if query in self.idxDict:\n",
    "            return self.idxDict[query]\n",
    "        return -1\n",
    "\n",
    "validGenres = ['Indie', 'Action', 'Casual', 'Adventure', 'Strategy', 'Simulation', 'RPG', 'Sports', 'Massively Multiplayer', 'Racing']\n",
    "genre2idx = Str2idx(validGenres)\n",
    "\n",
    "def LabelToIdx(data):\n",
    "    for idx in range(len(data)):\n",
    "        data[idx] = [genre2idx(x) for x in data[idx]]\n",
    "    return data\n",
    "\n",
    "ytrain, yvalid, ytest = LabelToIdx(ytrain), LabelToIdx(yvalid), LabelToIdx(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToMultiHot(data, numValues=10, bs=4096):\n",
    "    mat = np.zeros((len(data), numValues))\n",
    "    for idx in range(len(data)):\n",
    "        mat[idx, data[idx]] = 1\n",
    "\n",
    "    res = []\n",
    "    for idx in range(0, len(data), bs):\n",
    "        m = mat[idx:idx+bs]\n",
    "        res.append(torch.from_numpy(m).to(torch.float32).cuda())\n",
    "    return res\n",
    "\n",
    "ytrain, yvalid, ytest = ToMultiHot(ytrain), ToMultiHot(yvalid), ToMultiHot(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CsrToTorchSparse(csr, bs=4096):\n",
    "    res = []\n",
    "    for idx in range(0, csr.shape[0], bs):\n",
    "        c = csr[idx:idx+bs]\n",
    "        c = torch.sparse_csr_tensor(c.indptr, c.indices, c.data, c.shape, dtype=torch.float32)\n",
    "        res.append(c.cuda())\n",
    "    return res\n",
    "\n",
    "Xtrain = CsrToTorchSparse(Xtrain)\n",
    "Xvalid = CsrToTorchSparse(Xvalid)\n",
    "Xtest = CsrToTorchSparse(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for scipy sparse matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        super().__init__()\n",
    "        self.data = data                # CSR\n",
    "        self.targets = targets          # Dense\n",
    "        \n",
    "    def __getitem__(self, index:int):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SparseDataset(Xtrain, ytrain)\n",
    "valid_set = SparseDataset(Xvalid, yvalid)\n",
    "test_set = SparseDataset(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(20000, 4096), nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(4096, 512), nn.ReLU())\n",
    "        self.fc3 = nn.Sequential(nn.Linear(512, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "bs = 4096\n",
    "\n",
    "def Jaccard(pred, target, thresh=0.5):\n",
    "    pred = 1.0 * (pred > thresh)\n",
    "    u = torch.sum(pred * target, dim=1)\n",
    "    p = torch.sum(pred, dim=1)\n",
    "    t = torch.sum(target, dim=1)\n",
    "    precision = torch.mean(u / p).cpu().item()\n",
    "    recall = torch.mean(u / t).cpu().item()\n",
    "    return precision, recall\n",
    "\n",
    "@torch.no_grad()\n",
    "def val(model):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for data, target in valid_set:\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, target)\n",
    "        val_loss += loss.data\n",
    "        p, r = Jaccard(pred, target)\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "    val_loss = val_loss / len(valid_set)\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1 = 2 / (1/precision + 1/recall)\n",
    "    return val_loss, precision, recall, f1\n",
    "\n",
    "def train(model, optimizer, epoch, lr_scheduler=None, grad_clip=None):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0\n",
    "    for data, target in train_set:\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, target)\n",
    "        train_loss += loss.data\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if grad_clip: \n",
    "            nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "    if lr_scheduler:\n",
    "        lr_scheduler.step()\n",
    "    train_loss = train_loss / len(train_set)\n",
    "    val_loss, precision, recall, f1 = val(model)\n",
    "    end_time = time.time()\n",
    "    msg = f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val prec: {precision:.4f} | Val recall: {recall:.4f} | Val F1: {f1:.4f} | time: {end_time - start_time:.1f}\"\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No bias decay \n",
    "def create_param_groups(model):\n",
    "    group_decay = []\n",
    "    group_no_decay = []\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            if m.weight is not None:\n",
    "                group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "    assert(len(list(model.parameters())) == len(group_decay) + len(group_no_decay))\n",
    "    return [dict(params=group_decay), dict(params=group_no_decay, weight_decay=0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n",
    "\n",
    "EP = 300\n",
    "model = MyMLP().cuda()\n",
    "optimizer = optim.SGD(\n",
    "    create_param_groups(model),\n",
    "    weight_decay=1e-2,\n",
    "    lr = 1e-2\n",
    ")\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, EP)\n",
    "\n",
    "for ep in range(EP):\n",
    "    train(model, optimizer, ep, lr_scheduler, grad_clip=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
