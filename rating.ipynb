{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse import coo_matrix, csr_matrix, vstack\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "assert(torch.cuda.is_available())\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = torch.load('build/extracted+reg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Str2idx():\n",
    "    def __init__(self, myset) -> None:\n",
    "        self.idxDict = {}\n",
    "        idx = 0\n",
    "        for k in myset:\n",
    "            self.idxDict[k] = idx\n",
    "            idx += 1\n",
    "\n",
    "    def __call__(self, query):\n",
    "        if query in self.idxDict:\n",
    "            return self.idxDict[query]\n",
    "        return -1\n",
    "\n",
    "validLabels = list(set(ytrain))\n",
    "fit2idx = Str2idx(validLabels)\n",
    "\n",
    "def LabelToIdx(data, bs=4096):\n",
    "    return [fit2idx(x) for x in data]\n",
    "    # mat = [fit2idx(x) for x in data]\n",
    "    # res = []\n",
    "    # for idx in range(0, len(mat), bs):\n",
    "    #     m = mat[idx:idx+bs]\n",
    "    #     res.append(torch.Tensor(m).long().cuda())\n",
    "    # return res\n",
    "\n",
    "ytrain, yvalid, ytest = LabelToIdx(ytrain), LabelToIdx(yvalid), LabelToIdx(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit2idx.idxDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToOneHot(rawVec, numValues=5, bs=4096):\n",
    "    rawVec = np.array(rawVec)\n",
    "    oneHotMat = np.zeros((rawVec.shape[0], numValues))  # Initialize\n",
    "    oneHotMat[np.arange(rawVec.shape[0]), rawVec] = 1   # `numValues`-dim one hot\n",
    "    res = []\n",
    "    for idx in range(0, oneHotMat.shape[0], bs):\n",
    "        m = oneHotMat[idx:idx+bs]\n",
    "        res.append(torch.FloatTensor(m).cuda())\n",
    "    return res\n",
    "\n",
    "ytrain, yvalid, ytest = ToOneHot(ytrain), ToOneHot(yvalid), ToOneHot(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CsrToTorchSparse(csr, bs=4096):\n",
    "    res = []\n",
    "    for idx in range(0, csr.shape[0], bs):\n",
    "        c = csr[idx:idx+bs]\n",
    "        c = torch.sparse_csr_tensor(c.indptr, c.indices, c.data, c.shape, dtype=torch.float32)\n",
    "        res.append(c.cuda())\n",
    "    return res\n",
    "\n",
    "Xtrain = CsrToTorchSparse(Xtrain)\n",
    "Xvalid = CsrToTorchSparse(Xvalid)\n",
    "Xtest = CsrToTorchSparse(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for scipy sparse matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        super().__init__()\n",
    "        self.data = data                # CSR\n",
    "        self.targets = targets          # Dense\n",
    "        \n",
    "    def __getitem__(self, index:int):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SparseDataset(Xtrain, ytrain)\n",
    "valid_set = SparseDataset(Xvalid, yvalid)\n",
    "test_set = SparseDataset(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(1000, 512), nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(512, 100), nn.ReLU())\n",
    "        self.fc3 = nn.Sequential(nn.Linear(100, 5))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([2.35, 1.2, 0.45, 0.16, 0.09])).cuda()\n",
    "# criterion = nn.CrossEntropyLoss().cuda()\n",
    "bs = 4096\n",
    "\n",
    "def MSE(pred, target):\n",
    "    pred = 2 * (pred.argmax(dim=1) + 1).float()\n",
    "    target = 2 * (target.argmax(dim=1) + 1).float()\n",
    "    return torch.mean((pred - target)**2).cpu().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def val(model, dataset=valid_set):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    p = torch.Tensor([0, 0, 0, 0, 0])\n",
    "    u = torch.Tensor([0, 0, 0, 0, 0])\n",
    "    t = torch.Tensor([0, 0, 0, 0, 0])\n",
    "    mse = 0.0\n",
    "    for data, target in dataset:\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, target)\n",
    "        val_loss += loss.data\n",
    "        pred = nn.functional.one_hot(pred.argmax(dim=1), num_classes=5)\n",
    "        p += torch.sum(pred, dim=0).cpu()\n",
    "        u += torch.sum(pred*target, dim=0).cpu()\n",
    "        t += torch.sum(target, dim=0).cpu()\n",
    "        mse += MSE(pred, target)\n",
    "    val_loss = val_loss / len(dataset)\n",
    "    mse = mse / len(dataset)\n",
    "    return val_loss, p, u, t, mse\n",
    "\n",
    "def train(model, optimizer, epoch, lr_scheduler=None, grad_clip=None):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0\n",
    "    for data, target in train_set:\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, target)\n",
    "        train_loss += loss.data\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if grad_clip: \n",
    "            nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "    if lr_scheduler:\n",
    "        lr_scheduler.step()\n",
    "    train_loss = train_loss / len(train_set)\n",
    "    val_loss, p, u, t, mse = val(model)\n",
    "    f1 = 2 * u / (p + t)\n",
    "    end_time = time.time()\n",
    "    msg = f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | u: {u} | p: {p} | t: {t} | Val F1: {f1} | Val MSE: {mse} | time: {end_time - start_time:.1f}\"\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No bias decay \n",
    "def create_param_groups(model):\n",
    "    group_decay = []\n",
    "    group_no_decay = []\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            if m.weight is not None:\n",
    "                group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "    assert(len(list(model.parameters())) == len(group_decay) + len(group_no_decay))\n",
    "    return [dict(params=group_decay), dict(params=group_no_decay, weight_decay=0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n",
    "\n",
    "EP = 500\n",
    "model = MyMLP().cuda()\n",
    "optimizer = optim.SGD(\n",
    "    # create_param_groups(model),\n",
    "    model.parameters(),\n",
    "    weight_decay=5e-4,\n",
    "    lr = 0.5\n",
    ")\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, EP)\n",
    "\n",
    "for ep in range(EP):\n",
    "    train(model, optimizer, ep, lr_scheduler, grad_clip=None)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, p, u, t, mse = val(model, test_set)\n",
    "f1 = 2 * u / (p + t)\n",
    "msg = f\"Test Loss: {val_loss:.4f} | u: {u} | p: {p} | t: {t} | Val F1: {f1} | Val MSE: {mse}\"\n",
    "print(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
